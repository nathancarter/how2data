
One measure of the goodness of fit of a model is the standard error of its
estimates.  If the actual values are $y_i$ and the estimates are $\hat y_i$,
the definition of this quantity is as follows, for $n$ data points.

$$ \sigma_{\text{est}} = \sqrt{ \frac{ \sum (y_i-\hat y_i)^2 }{ n } } $$

If we've fit a linear model, how do we compute the standard error of its estimates?
